{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIDL A02 Project: Gesture2Emote\n",
    "##### Amalia Contiero Syropoulou - mscaidl-0059 - 16/9/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, LSTM, Bidirectional, Input, GlobalAveragePooling1D, Attention, Concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_dir = '/content/drive/My Drive/Numpy_Data/'\n",
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize as sets so we dont save duplicates\n",
    "subject_ids = set()\n",
    "rec_types = set()\n",
    "gesture_classes = set()\n",
    "body_segments = set()\n",
    "trials = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(npy_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        parts = filename.split('_')\n",
    "        subject_id = parts[0][3:]\n",
    "        rec_type = parts[1]\n",
    "        gesture_class = parts[2] if rec_type != 'Idle' else 'Idle'\n",
    "        trial_number = parts[3].split('-')[-1] if rec_type != 'Idle' else parts[2].split('-')[-1]\n",
    "        segment = parts[-1].replace('.npy', '')\n",
    "        \n",
    "        subject_ids.add(subject_id)\n",
    "        rec_types.add(rec_type)\n",
    "        gesture_classes.add(gesture_class)\n",
    "        trials.add(trial_number)\n",
    "        body_segments.add(segment)\n",
    "\n",
    "        # Load\n",
    "        filepath = os.path.join(npy_dir, filename)\n",
    "        imu_data = np.load(filepath)\n",
    "\n",
    "        # Save in an organized manner\n",
    "        if subject_id not in data:\n",
    "            data[subject_id] = {\n",
    "                'Segmented': {},\n",
    "                'Continuous': {},\n",
    "                'Idle': {},\n",
    "            }\n",
    "        if rec_type == 'Segmented' or rec_type == 'Continuous':\n",
    "            if gesture_class not in data[subject_id][rec_type]:\n",
    "                data[subject_id][rec_type][gesture_class] = {}\n",
    "            if trial_number not in data[subject_id][rec_type][gesture_class]:\n",
    "                data[subject_id][rec_type][gesture_class][trial_number] = {}\n",
    "            data[subject_id][rec_type][gesture_class][trial_number][segment] = imu_data\n",
    "                    \n",
    "        elif rec_type == 'Idle':\n",
    "            if 'Idle' not in data[subject_id][rec_type]:\n",
    "                data[subject_id][rec_type]['Idle'] = {}\n",
    "            if trial_number not in data[subject_id][rec_type]['Idle']:\n",
    "                data[subject_id][rec_type]['Idle'][trial_number] = {}\n",
    "            data[subject_id][rec_type]['Idle'][trial_number][segment] = imu_data\n",
    "\n",
    "\n",
    "print(\"Data loading complete.\")\n",
    "print(f\"{len(subject_ids)} Subject IDs: {sorted(subject_ids)}\")\n",
    "print(f\"{len(rec_types)} Recording Types: {sorted(rec_types)}\")\n",
    "print(f\"{len(gesture_classes)} Gesture Classes: {sorted(gesture_classes)}\")\n",
    "print(f\"{len(trials)} Trials: {min(trials)} to {max(trials)}\")\n",
    "print(f\"{len(body_segments)} Body Segments: {sorted(body_segments)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gesture2Emote Dataset  \n",
    "\n",
    "The dataset comprises motion capture recordings using the Xsens Awinda system. It includes:\n",
    "\n",
    "- **Subjects**: 5 individuals\n",
    "- **Gesture Classes**: 10 distinct gestures, plus an additional 'Idle' class\n",
    "- **Recording Types**:\n",
    "  - **Segmented**: Single execution of each gesture class, recorded 50 times\n",
    "  - **Continuous**: 2-minute of repeated execution of each gesture class, recorded once\n",
    "  - **Idle**: 10-minute session of non-classified gesture/idle activity for each subject\n",
    "- **Body Segments**:\n",
    "  - **HAND**\n",
    "  - **fARM** (forearm/wrist)\n",
    "  - **uARM** (upper arm)\n",
    "\n",
    "The subjects were given a vague description and demonstration of the gesture to be performed, but were allowed to introduce their own variances. The gesture classes are as follows:  \n",
    " - **Angry** : raise and shake your fist in an angry manner\n",
    " - **Cheer** : raise your hand high in a celebratory manner\n",
    " - **Clap** : clap twice\n",
    " - **Come** : move your arm to call someone to come over\n",
    " - **Crazy** : raise hand beside head and do a circle with your index finger\n",
    " - **Facepalm** : facepalm motion without touching face\n",
    " - **Point** :  close elbow and then extend it straight to point somewhere\n",
    " - **Salute** : salute with raised hand tilted on forehead level, like a soldier\n",
    " - **ThumbsUp** : raise hand and then bring it down for a thumbs up\n",
    " - **Wave** : raise hand and wave\n",
    "\n",
    "The dataset was captured using MT Manager and then exported to ASCII .txt and npy files with custom scripts. The files include columns of orientation quaternion and euler acceleration data, while rows represent the packets received in 60FPS. The IMUs were initialized with a heading, orientation and alignment reset before each subject's data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_segmented_length(data, rec_type='Segmented'):\n",
    "    max_length = 0\n",
    "    \n",
    "    for subject_id, rec_types in data.items():\n",
    "        if rec_type not in rec_types:\n",
    "            continue\n",
    "        \n",
    "        for gesture_class, trials in rec_types[rec_type].items():\n",
    "            for trial_number, segments in trials.items():\n",
    "                # Check each body segment\n",
    "                for segment in ['HAND', 'fARM', 'uARM']:\n",
    "                    if segment in segments:\n",
    "                        segment_length = len(segments[segment])\n",
    "                        \n",
    "                        if segment_length > max_length:\n",
    "                            max_length = segment_length\n",
    "    \n",
    "    return max_length\n",
    "\n",
    "# Find the longest length for segmented data\n",
    "max_length = find_max_segmented_length(data, rec_type='Segmented')\n",
    "print(f\"Longest length for segmented data: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_motion(imu_data):\n",
    "    quaternions = imu_data[:, :4]  # Columns 0 to 3 for quaternions\n",
    "    accelerations = imu_data[:, 4:]  # Columns 4 to 6 for accelerations\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(quaternions, label=['q0', 'q1', 'q2', 'q3'])\n",
    "    plt.title('IMU Orientation Quaternions')\n",
    "    plt.xlabel('Motion duration (1/60 s)')\n",
    "    plt.ylabel('quaternion')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(accelerations, label=['x', 'y', 'z'])\n",
    "    plt.title('IMU Euler Accelerations')\n",
    "    plt.xlabel('Motion duration (1/60 s)')\n",
    "    plt.ylabel('Acceleration (m/s^2)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example plots of the motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_motion = data['005']['Segmented']['Wave']['000']['HAND']\n",
    "continuous_motion_2 = data['005']['Continuous']['Wave']['000']['HAND']\n",
    "print(segmented_motion.shape)\n",
    "plot_motion(segmented_motion)\n",
    "plot_motion(continuous_motion_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idle_motion_3 = data['005']['Idle']['Idle']['000']['HAND']\n",
    "plot_motion(idle_motion_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_sliding_windows(data, window_size=380, step_size=190):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for subject_id, rec_types in data.items():\n",
    "        for rec_type, gestures in rec_types.items():\n",
    "            #if rec_type == 'Segmented':\n",
    "            for gesture_class, trials in gestures.items():\n",
    "                for trial_number, segments in trials.items():\n",
    "                    if 'HAND' in segments and 'fARM' in segments and 'uARM' in segments:\n",
    "                        # to avoid packet loss\n",
    "                        max_length = max(len(segments['HAND']), len(segments['fARM']), len(segments['uARM']))\n",
    "\n",
    "                        hand_data = np.pad(segments['HAND'], ((0, max_length - len(segments['HAND'])), (0, 0)), mode='constant')\n",
    "                        farm_data = np.pad(segments['fARM'], ((0, max_length - len(segments['fARM'])), (0, 0)), mode='constant')\n",
    "                        uarm_data = np.pad(segments['uARM'], ((0, max_length - len(segments['uARM'])), (0, 0)), mode='constant')\n",
    "\n",
    "                        imu_data = np.concatenate([hand_data, farm_data, uarm_data], axis=1) \n",
    "\n",
    "                        # ensure at least 1 window fit\n",
    "                        if len(imu_data) < window_size:\n",
    "                            imu_data = np.pad(imu_data, ((0, window_size - len(imu_data)), (0, 0)), mode='constant')\n",
    "\n",
    "                        # Normalize features\n",
    "                        scaler = StandardScaler()\n",
    "                        imu_data_scaled = scaler.fit_transform(imu_data)\n",
    "\n",
    "                        # Apply sliding window\n",
    "                        for start in range(0, len(imu_data_scaled) - window_size + 1, step_size):\n",
    "                            end = start + window_size\n",
    "                            window = imu_data_scaled[start:end]\n",
    "                            \n",
    "                            X.append(window)\n",
    "                            y.append(gesture_class)\n",
    "    \n",
    "   \n",
    "    X = np.array(X)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    labels = list(sorted(set(y)))\n",
    "    y_encoded = np.array([labels.index(label) for label in y])\n",
    "    y_one_hot = to_categorical(y_encoded, num_classes=len(labels))\n",
    "    \n",
    "    return X, y_one_hot, labels\n",
    "\n",
    "window_size = 380  \n",
    "step_size = int(window_size/2)  # 50% overlap of window_size\n",
    "\n",
    "X, y_one_hot, labels = preprocess_data_sliding_windows(data, window_size, step_size)\n",
    "\n",
    "print(f\"Feature shape: {X.shape}\")  # (num_samples, window_size, num_features)\n",
    "print(f\"Label shape: {y_one_hot.shape}\")  # (num_samples, num_classes)\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network 1\n",
    "def model_cnn_1(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Convolutional Block 1\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        # Convolutional Block 2\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        # Convolutional Block 3\n",
    "        Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        # Flatten and Fully Connected Layer\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        # Output Layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network 3\n",
    "def model_cnn_2(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Convolutional Block 1\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Convolutional Block 2\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Convolutional Block 3\n",
    "        Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        # Convolutional Block 4\n",
    "        Conv1D(filters=512, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        # Flatten and Fully Connected Layers\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        # Output Layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn_3(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # First Block\n",
    "        Conv1D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Second Block\n",
    "        Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third Block\n",
    "        Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Fourth Block\n",
    "        Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Flatten and Fully Connected Layer\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Output Layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_cnn(X_train, y_train, X_test, y_test, model_cnn, labels, save_dir='best_model', initial_batch_size=32, epochs_per_step=30, steps=3):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = os.path.join(save_dir, f'cnn_best_model.keras')\n",
    "\n",
    "    # Callbacks\n",
    "    save_model = ModelCheckpoint(\n",
    "        model_path, \n",
    "        save_best_only=True, \n",
    "        monitor='val_loss', \n",
    "        mode='min', \n",
    "        verbose=0\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10\n",
    "    )\n",
    "    reducelr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.8, \n",
    "        patience=3, \n",
    "        verbose=0, \n",
    "        mode='min', \n",
    "        min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    # Build the CNN model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])  # (sequence_length, num_features)\n",
    "    num_classes = y_train.shape[1]\n",
    "    cnn_model = model_cnn(input_shape, num_classes)\n",
    "\n",
    "    # Training with varying batch sizes\n",
    "    full_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "    batch_size = initial_batch_size\n",
    "    for step in range(steps):\n",
    "        print(f\"\\nTraining with batch size: {batch_size}\")\n",
    "\n",
    "        history = cnn_model.fit(\n",
    "            X_train, y_train, \n",
    "            epochs=epochs_per_step, \n",
    "            validation_split=0.2, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            verbose=0, \n",
    "            callbacks=[save_model, early_stopping, reducelr]\n",
    "        )\n",
    "\n",
    "        # Append current history to full_history for train/val accuracy plot\n",
    "        for key in full_history.keys():\n",
    "            full_history[key].extend(history.history[key])\n",
    "\n",
    "        batch_size *= 2\n",
    "\n",
    "    # Load the best model\n",
    "    cnn_model.load_weights(model_path)\n",
    "    test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Predict the classes\n",
    "    y_pred = cnn_model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Create  and plot confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(f'{model_cnn.__name__} Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(full_history['loss'], label='Train Loss')\n",
    "    plt.plot(full_history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'{model_cnn.__name__} Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(full_history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(full_history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'{model_cnn.__name__} Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate F1 Score, Precision, and Recall\n",
    "    f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "    report = classification_report(y_true, y_pred_classes, target_names=labels)\n",
    "    \n",
    "    print(f\"F1 Score (Weighted): {f1:.4f}\")\n",
    "    print(f\"Precision (Weighted): {precision:.4f}\")\n",
    "    print(f\"Recall (Weighted): {recall:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    return test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_1 = train_and_evaluate_cnn(X_train, y_train, X_test, y_test, model_cnn_1,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_2 = train_and_evaluate_cnn(X_train, y_train, X_test, y_test, model_cnn_2,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_3 = train_and_evaluate_cnn(X_train, y_train, X_test, y_test, model_cnn_3,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {\n",
    "    'model_cnn_1': test_acc_1,\n",
    "    'model_cnn_2': test_acc_2,\n",
    "    'model_cnn_3': test_acc_3\n",
    "}\n",
    "\n",
    "best_model = max(accuracies, key=accuracies.get)\n",
    "best_accuracy = accuracies[best_model]\n",
    "\n",
    "print(f\"Our best performing CNN was {best_model} with test accuracy of {best_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model_1(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model_2(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model_3(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # First LSTM Layer with BatchNormalization\n",
    "    x = LSTM(128, return_sequences=True)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Second LSTM Layer with BatchNormalization\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Dense Layers\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 380\n",
    "step_size = 190\n",
    "input_shape = (window_size, 21) \n",
    "num_classes = len(labels)  \n",
    "\n",
    "X, y_one_hot, labels = preprocess_data_sliding_windows(data, window_size, step_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate F1 Score, Precision, and Recall\n",
    "    f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "    report = classification_report(y_true, y_pred_classes, target_names=labels)\n",
    "    \n",
    "    print(f\"F1 Score (Weighted): {f1:.4f}\")\n",
    "    print(f\"Precision (Weighted): {precision:.4f}\")\n",
    "    print(f\"Recall (Weighted): {recall:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    report = classification_report(y_true_classes, y_pred_classes, target_names=labels)\n",
    "    print(report)\n",
    "\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=10, model_name='model'):\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(f'{model_name}_best.keras', \n",
    "                                 monitor='val_loss', \n",
    "                                 save_best_only=True, \n",
    "                                 mode='min', \n",
    "                                 verbose=1)\n",
    "    \n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=epochs, \n",
    "                        batch_size=32, \n",
    "                        validation_split=0.2, \n",
    "                        verbose=0, \n",
    "                        callbacks=[checkpoint])\n",
    "    \n",
    "    \n",
    "    evaluate_model(model, X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Training LSTM Model 1:\")\n",
    "model_1 = lstm_model_1(input_shape, num_classes)\n",
    "train_and_evaluate(model_1, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Training LSTM Model 2:\")\n",
    "model_2 = lstm_model_2(input_shape, num_classes)\n",
    "train_and_evaluate(model_2, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Training LSTM Model 3:\")\n",
    "model_3 = lstm_model_3(input_shape, num_classes)\n",
    "train_and_evaluate(model_3, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Model (CNN, LSTM, Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Convolutional Layer\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Bidirectional LSTM Layer\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Another Bidirectional LSTM Layer\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Attention Mechanism (proper usage)\n",
    "    attention = Attention()([x, x])\n",
    "    x = Concatenate()([x, attention])\n",
    "    \n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Dense Layers\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LSTM Model 4:\")\n",
    "model_4 = hybrid_model(input_shape, num_classes)\n",
    "train_and_evaluate(model_4, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
